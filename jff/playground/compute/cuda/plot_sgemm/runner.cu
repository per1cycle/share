#include <iostream>
#include <string>
#include <iomanip>
#include <ctime>
#include <sstream>
#include <assert.h>

#include "common.cuh"

#include "kernel/sgemm_naive.cuh"
#include "kernel/sgemm_global_memory_coalescing.cuh"
#include "kernel/sgemm_share_memory_caching.cuh"
#include "kernel/sgemm_1d_tiling.cuh"
#include "kernel/sgemm_2d_tiling.cuh"

#include <cublas_v2.h>
#include <cuda_runtime.h>

int kernel_num = 6;
const int loop = 5;
const int shape_num = 20;
int shape[shape_num];
std::string marker = ".ov^<>s*P+xD";

// todo
namespace init
{

};

namespace generator
{
    
};

void gen_shape()
{
    for(int i = 0; i < shape_num; i ++)
    {
        shape[i] = (i + 1) * 256;
    }
}

std::string current_time()
{
    // Get current time
    std::time_t now = std::time(nullptr);
    std::tm* localTime = std::localtime(&now);

    // Create a string stream to format the time
    std::ostringstream oss;
    oss << std::put_time(localTime, "%Y-%m-%d-%H:%M:%S");

    return oss.str();
}

void save_fig()
{
    std::cout
            << "plt.legend()" << std::endl
            << "plt.savefig(" << "\"" << current_time() << ".png\"" << ")" << std::endl;
}

void import()
{
    std::cout 
            << "# Automatic generated by the runner, do not edit." << std::endl
            << "import numpy as np" << std::endl
            << "import matplotlib.pyplot as plt" << std::endl
            << "from matplotlib.pyplot import MultipleLocator" << std::endl
            << "plt.figure(figsize=(20, 12))" << std::endl
            << "plt.xlabel('Matrix Size N (for NxN matrices)')" << std::endl
            << "plt.ylabel('Performance (GFLOPs/s)')" << std::endl
            << "plt.title('SGEMM Performance Comparison')" << std::endl
            << "plt.grid(True)" << std::endl
            << "x_major_locator = MultipleLocator(256)" << std::endl
            << "plt.gca().xaxis.set_major_locator(x_major_locator)" << std::endl;

}

template <typename T>
void py_arr(T *arr, int arr_size, bool is_arg)
{
    std::cout << '[';
    for(int i = 0; i < arr_size; i ++)
    {
        std::cout << arr[i];
        if(i != arr_size - 1)
            std::cout << ',';
    }
    std::cout << ']';
    if(is_arg) 
        std::cout << ',';
}

void py_idx_arr(int arr_size)
{
    int *tmp = (int*) malloc(sizeof(int) * arr_size);
    for(int i = 0; i < arr_size; i ++) 
        tmp[i] = (i + 1);
    py_arr<int>(tmp, arr_size, false);
    free((void*)tmp);
}

inline void py_plt_start()
{
    std::cout << "plt.plot(" << std::endl;
}

inline void py_plt_end()
{
    std::cout << ")" << std::endl;
}

void select_marker(int i, bool is_arg)
{
    assert(i < marker.size());
    std::cout << "marker='" << marker[i] << "'";
    if(is_arg)
        std::cout << ',';
}

std::string type_to_kernel_name(int t)
{
    switch (t)
    {
    case 0:
    {
        return "cuBlas Implementation";
    }
    case 1:
    {
        return "Naive Implementation";
    }
    case 2:
    {
        return "Global Memory Coalescing";
    }
    case 3:
    {
        return "Shared Memory Cache-Blocking";
    }
    case 4:
    {
        return "1D Tiling";
    }
    case 5:
    {
        return "2D Tiling";
    }
    }
    return "Unknown kernel type";
}

void add_label(std::string kernel, bool is_arg)
{
    std::cout << "label=" << "\"" << kernel << "\"";
    if(is_arg)
        std::cout << ',';
}

void run_kernel(int kernel_type, int N, int M, int K, float *d_a, float *d_b, float *d_c, float alpha, float beta)
{
    const int BLK = 32;

    switch (kernel_type)
    {
        case 0:
        {
            cublasHandle_t handle; // cuBLAS context
            cublasCreate(&handle);
            cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N, N, N, N, &alpha, d_b, N,
                d_a, N, &beta, d_c, N);
            break;
        }
        case 1: // naive sgemm
        {
            // fix typo.
            dim3 grid_dim = {N / BLK, N / BLK, 1};
            dim3 blk_dim = {BLK, BLK, 1};
            sgemm_naive<<<grid_dim, blk_dim>>>(N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);

            break;
        }
        case 2:
        {
            dim3 grid_dim = {N / BLK, N / BLK};
            dim3 blk_dim = {BLK * BLK};

            sgemm_global_memory_coalescing<BLK><<<grid_dim, blk_dim>>>(N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);
            break;
        }
        case 3:
        {
            dim3 grid_dim = {N / BLK, N / BLK};
            dim3 blk_dim = {BLK * BLK};

            sgemm_share_memory_caching<BLK><<<grid_dim, blk_dim>>>(N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);
            break;
        }
        case 4:
        {
            const int BLK_N = 64;
            const int BLK_M = 8;
            const int BLK_K = 64;
            const int THREAD_N = 8;
            dim3 grid_dim = {N / BLK_N, N / BLK_K};
            dim3 blk_dim = {(BLK_N * BLK_K) / THREAD_N};

            sgemm_1d_tiling<BLK_N, BLK_M, BLK_K, THREAD_N><<<grid_dim, blk_dim>>>(N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);

            break;
        }
        case 5:
        {
            const int BLK_N = 128;
            const int BLK_M = 8;
            const int BLK_K = 128;
            const int THREAD_N = 8;
            const int THREAD_K = 8;
            dim3 grid_dim = {N / BLK_N, K / BLK_K};
            dim3 blk_dim = {(BLK_N * BLK_K) / (THREAD_N * THREAD_K)};

            sgemm_2d_tiling<BLK_N, BLK_M, BLK_K, THREAD_N, THREAD_K><<<grid_dim, blk_dim>>>(N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);

            break;
        }
    }
}

int main()
{
    gen_shape();
    import();
    /**
     * kernel type:
     * 0 cublas
     * 1 naive
     * 2 global opt
     * 3 share mem opt
     */
    for(int t = 0; t < kernel_num; t ++)
    {
        std::cout << "# Automatically generated by the runner, do not edit." << std::endl;
        std::cout << "# Run kernel: " << type_to_kernel_name(t) << std::endl;

        py_plt_start();
        float *gflops_arr = (float*) malloc(sizeof(float) * shape_num);
        
        // loop shape and get average of kernel performance.
        for(int i = 0; i < shape_num; i ++)
        {
            uint N = shape[i];
            size_t size = sizeof(float) * N * N;
            float flop = 1.0 * N * N * (2 * N + 1);
    
            float *h_a, *h_b, *h_c;
            h_a = (float*) malloc(sizeof(float) * size);
            h_b = (float*) malloc(sizeof(float) * size);
            h_c = (float*) malloc(sizeof(float) * size);
            memset(h_c, 0, size);

            float *d_a, *d_b, *d_c;
            cudaMalloc((void **)&d_a, size * sizeof(float));
            cudaMalloc((void **)&d_b, size * sizeof(float));
            cudaMalloc((void **)&d_c, size * sizeof(float));
    
            cudaMemcpy(d_a, h_a, size * sizeof(float), cudaMemcpyHostToDevice);
            cudaMemcpy(d_b, h_b, size * sizeof(float), cudaMemcpyHostToDevice);
            cudaMemcpy(d_c, h_c, size * sizeof(float), cudaMemcpyHostToDevice);

            float elapse_sum = 0.0f;
            for(int lo = 0; lo < loop; lo ++)
            {
                float elapse;
                cudaEvent_t start, stop;
                cudaEventCreate(&start);
                cudaEventCreate(&stop);
                cudaEventRecord(start, 0);
                
                run_kernel(t, N, N, N, d_a, d_b, d_c, 1.0f, 0.0f);

                cudaEventRecord(stop, 0);
                cudaEventSynchronize(stop);
                cudaEventElapsedTime(&elapse, start, stop);
                
                elapse_sum += elapse / 1000.0f;
            }
            
            float gflop = flop / 1000000000.0f;
            gflops_arr[i] = gflop / (elapse_sum / loop);

            cudaMemcpy(h_c, d_c, size * sizeof(float), cudaMemcpyDeviceToHost);

            cudaFree(d_a);
            cudaFree(d_b);
            cudaFree(d_c);

            free((void*)h_a);
            free((void*)h_b);
            free((void*)h_c);
        }
        
        py_arr<int>(shape, shape_num, true);
        py_arr<float>(gflops_arr, shape_num, true);
        add_label(type_to_kernel_name(t), true);
        select_marker(t, false);
        py_plt_end();
        free((void*)gflops_arr);
    }

    save_fig();

    return 0;
}